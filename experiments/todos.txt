Challenges/next steps: 
1. Feature Fusion Across Complexes: Design attention mechanisms to weight contributions from 0-, 1-, and 2-complexes.
2. Validation: Compare your SCN-PDE against MP-PDE on metrics like solution accuracy and training stability.
3. Test your SCN-PDE on canonical PDEs (e.g., Burgers’, Navier-Stokes).
4. 

#swish 
# Unlike ReLU, which has a kink at 0, Swish is infinitely differentiable. Its slight non-monotonic “bump” around x≈−1 
# can help the network learn richer mappings—and in practice often yields lower training loss on deep architectures.
# converge faster and generalize slightly better than ReLU or tanh, gains translate into more stable message/update mappings and improved PDE-solving accuracy.
# Swish is a smooth, non-monotonic activation function that has been shown to outperform ReLU in some cases.
# Swish is defined as f(x) = x * sigmoid(x), where sigmoid(x) = 1 / (1 + exp(-x)).
# Self-gating.
# The factor σ(βx) acts like a learned gate that gradually “opens” for positive x and “closes” for negative 
# x, but in a smooth, data-driven way. That can improve gradient flow (fewer dead neurons) and let the network adaptively choose how much of each input to pass through.





######################################################################################
######################################################################################
# version 1 of SCN-PDE solver

# SCN conv for k-complex calculation
class SimplicialConv(MessagePassing):
    def __init__(self, in_channels, out_channels, boundary_index, aggr='mean'):
        super().__init__(aggr=aggr)
        self.boundary_index = boundary_index  # incidence map of simplices
        self.lin = nn.Linear(in_channels, out_channels)
    def forward(self, x):                     # x: [num_simplices, in_channels]
        return self.propagate(self.boundary_index, x=x)
    def message(self, x_j):
        return self.lin(x_j)

###______________________________________________________________________
# MLP‐encoder + CNN‐decoder pattern ensures network to keep the original feature dimensions 
# at the PDE-solver while leveraging the SCN hidden representation internally.
###______________________________________________________________________
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn.conv import MessagePassing
from torch_scatter import scatter_add

# SCN modules for k-complex processing
class SimplicialConv(MessagePassing):
    def __init__(self, in_channels, out_channels, boundary_index, aggr='mean'):
        super().__init__(aggr=aggr)
        self.boundary_index = boundary_index  # incidence map of simplices
        self.lin = nn.Linear(in_channels, out_channels)
    def forward(self, x):
        # x: [num_simplices, in_channels]
        return self.propagate(self.boundary_index, x=x)
    def message(self, x_j):
        return self.lin(x_j)

# Main SCN-PDE layer combining 0-,1-,2-,3-complexes
# Hierarchical Architecture:
# Existing codes often process data at a single topological level (e.g., nodes only).
# we add modules to process k-complexes (e.g., edges, triangles) and fuse their features. For example:
class SCNLayer(nn.Module):
    def __init__(self, node_dim, edge_dim, tri_dim, tet_dim, hidden_dim,
                 inc_0_1, inc_1_2, inc_2_3):
        super().__init__()
        # Encoder: MLP to project raw features to hidden_dim
        self.node_encoder = nn.Sequential(
            nn.Linear(node_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        # Convos on each simplex order on encoded features
        self.conv0 = nn.Conv1d(node_dim, hidden_dim, kernel_size=1) # 0-simplex
        self.conv1 = SimplicialConv(hidden_dim, hidden_dim, boundary_index=inc_0_1)  # edges→nodes
        self.conv2 = SimplicialConv(hidden_dim, hidden_dim, boundary_index=inc_1_2)  # triangles→edges
        self.conv3 = SimplicialConv(hidden_dim, hidden_dim, boundary_index=inc_2_3)  # tets→triangles

        # Decoder: 1D CNN to map hidden_dim back to node_dim
        self.node_decoder = nn.Conv1d(hidden_dim, node_dim, kernel_size=1)
        # Fusor for combined hidden signals
        self.fuse = nn.Linear(4*hidden_dim, hidden_dim)

    def forward(self, x0, x1, x2, x3):
        # x0: raw node features, x1: edge feats, x2: tri feats, x3: tet feats
        # # 1) Encode node features
        # x0_enc = self.node_encoder(x0)            # -> [N, hidden_dim]
        # # 2) SCN convolutions
        # h0 = self.conv0(x0_enc)                   # -> [N, hidden_dim]
        # h1 = self.conv1(x1)                       # -> [E, hidden_dim]
        # h2 = self.conv2(x2)                       # -> [T, hidden_dim]
        # # 3) Pool higher-order signals back to nodes
        # pool1 = scatter_add(h1, self.conv1.boundary_index[0], dim=0, dim_size=x0.size(0))
        # pool2 = scatter_add(h2, self.conv2.boundary_index[0], dim=0, dim_size=x0.size(0))
        # pool3 = torch.zeros_like(h0)  # placeholder for tetrahedra pooling
        # # 4) Fuse hidden representations
        # hidden = self.fuse(torch.cat([h0, pool1, pool2, pool3], dim=-1))
        # # 5) Decode back to original node feature dimension
        # decoded = self.node_decoder(hidden.unsqueeze(-1)).squeeze(-1)
        # return F.relu(decoded)
        
        # Encode features
        h3 = self.tet_encoder(x3)
        h2 = self.tri_encoder(x2)
        h1 = self.edge_encoder(x1)
        h0 = self.node_encoder(x0)
        
        # Process 3→2→1→0
        # tets→triangles
        h3 = self.conv3(h3)  
        h2 = h2 + scatter_add(h3, self.conv3.boundary_index[0], dim_size=h2.size(0))
        
        # triangles→edges
        h2 = self.conv2(h2)  
        h1 = h1 + scatter_add(h2, self.conv2.boundary_index[0], dim_size=h1.size(0))
        
        # edges→nodes
        h1 = self.conv1(h1)  
        h0 = h0 + scatter_add(h1, self.conv1.boundary_index[0], dim_size=h0.size(0))
        return self.decoder(h0)

# SCN-PDE Solver net
class SCNPDEModel(nn.Module):
    def __init__(self, mesh, time_steps, feature_dims, hidden_dim):
        super().__init__()
        # build complexes incidence
        inc_0_1, inc_1_2, inc_2_3 = build_space_time_complex(mesh, time_steps)
        self.layer = SCNLayer(
            node_dim=feature_dims['node'],
            edge_dim=feature_dims['edge'],
            tri_dim=feature_dims['triangle'],
            tet_dim=feature_dims['tetra'],
            hidden_dim=hidden_dim,
            inc_0_1=inc_0_1,
            inc_1_2=inc_1_2,
            inc_2_3=inc_2_3
        )
        self.final = nn.Linear(hidden_dim, feature_dims['node'])
    def forward(self, x0, x1, x2, x3):
        h = self.layer(x0, x1, x2, x3)
        return self.final(h)

# Utilities
def build_space_time_complex(mesh, time_steps):
    # mesh: 2D spatial mesh with .edges, .triangles, .nodes
    # returns PyG-style boundary incidence indexes
    # inc_0_1: mapping edges->nodes, shape [2, num_edges]
    inc_0_1 = mesh.edge_index
    inc_1_2 = mesh.tri_edge_index
    # for 2->3 (triangles->tets) build by extruding over time
    inc_2_3 = extrude_triangles_to_tets(mesh.triangles, time_steps)
    return inc_0_1, inc_1_2, inc_2_3

def extrude_triangles_to_tets(mesh, triangles, T):
    # create tetrahedrons by connecting spatial triangles across time
    tet_index = []
    for t in range(T-1):
        offset = t * len(mesh.nodes)
        next_offset = (t+1)*len(mesh.nodes)
        for tri in triangles:
            a,b,c = tri
            tet = [a+offset, b+offset, c+offset, a+next_offset]
            tet_index.append(tet)
    # flatten to [4, num_tets]
    return torch.tensor(tet_index).t()

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn.conv import MessagePassing

class EdgeProcessor(nn.Module):
    def __init__(self, in_dim, hidden_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    def forward(self, edge_feats):
        return self.mlp(edge_feats)

class TriangleProcessor(nn.Module):
    def __init__(self, in_dim, hidden_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    def forward(self, tri_feats):
        return self.mlp(tri_feats)

class SCNMessagePassing(MessagePassing):
    def __init__(self, in_dim, hidden_dim, k_complex_dims):
        super().__init__(aggr='mean')
        # existing MLP kernel can be replaced or combined
        self.node_mlp = nn.Sequential(
            nn.Linear(2*in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        # modules for k-complex processing
        self.edge_processor = EdgeProcessor(k_complex_dims['edge'], hidden_dim)
        self.tri_processor  = TriangleProcessor(k_complex_dims['triangle'], hidden_dim)
        # fusion of node + k-complex
        self.fuse = nn.Linear(3*hidden_dim, hidden_dim)

    def forward(self, x, edge_index, edge_feats, tri_index, tri_feats):
        # x: node features  [N, in_dim]
        # edge_index: [2, E]
        # edge_feats:    [E, k_complex_dims['edge']]
        # tri_index:    [3, T]
        # tri_feats:    [T, k_complex_dims['triangle']]

        # 1) Message passing for node-to-node
        out = self.propagate(edge_index, x=x)  # uses message() below
        # 2) Process k-complex features
        h_edge = self.edge_processor(edge_feats)       # -> [E, hidden_dim]
        h_tri  = self.tri_processor(tri_feats)         # -> [T, hidden_dim]
        # 3) Pool k-complex features back to nodes
        # e.g., average edges per node, sum triangles per node
        num_nodes = x.size(0)
        edge_pool = torch.zeros(num_nodes, hidden_dim, device=x.device)
        tri_pool  = torch.zeros(num_nodes, hidden_dim, device=x.device)
        # scatter add
        src, dst = edge_index
        edge_pool = edge_pool.index_add_(0, src, h_edge)
        tri_pool  = tri_pool.index_add_(0, tri_index.view(-1), h_tri.repeat(3,1))

        # 4) Fuse
        fused = self.fuse(torch.cat([out, edge_pool, tri_pool], dim=-1))
        return F.relu(fused)

    def message(self, x_i, x_j):
        # x_i, x_j: [E, in_dim]
        return self.node_mlp(torch.cat([x_i, x_j], dim=-1))







######################################################################################
######################################################################################
# version 2 of SCN-PDE solver



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn.conv import MessagePassing
from torch_scatter import scatter_add

# --------------------------------------------------
# SCNKernel: Implements 0-,1-,2-,3-simplex Green's-fn approximators
# --------------------------------------------------
class SCNKernel(nn.Module):
    def __init__(self, hidden_dim, inc_maps, Z_maps):
        super().__init__()
        # Incident matrices
        self.A01 = inc_maps['0_1']  # nodes↔edges
        self.A12 = inc_maps['1_2']  # edges↔triangles
        self.A02 = inc_maps['0_2']  # nodes↔triangles
        self.A23 = inc_maps['2_3']  # triangles↔tets
        
        # Normalization maps (diagonal matrices as vectors)
        self.Z10 = Z_maps['1_0']  # edges→nodes
        self.Z20 = Z_maps['2_0']  # triangles→nodes
        self.Z32 = Z_maps['3_2']  # tets→triangles

        # Learnable parameters
        self.alpha = nn.Parameter(torch.tensor(0.5))  # learnable trade-off
        self.Q1 = nn.Linear(hidden_dim, hidden_dim)
        self.Q2 = nn.Linear(hidden_dim, hidden_dim)
        self.Q3 = nn.Linear(hidden_dim, hidden_dim)
        self.act = Swish(beta=1)  # Swish activation

    def forward(self, X0_enc, X1_enc, X2_enc, X3_enc):
        # Direct kernels (0/2-simplex)
        K0 = torch.sparse.mm(self.A01, torch.sparse.mm(X0_enc, self.A01.t()))
        K2 = torch.sparse.mm(self.A02, torch.sparse.mm(X0_enc, self.A02.t()))
        Dir = K0 + K2
        
        # Indirect kernels (1/2/3-simplex)
        K1 = torch.sparse.mm(self.A01, self.Z10 * self.Q1(X1_enc))
        K2i = torch.sparse.mm(self.A02, self.Z20 * self.Q2(X2_enc))
        T2 = torch.sparse.mm(self.A23, self.Q3(X3_enc))
        K3 = torch.sparse.mm(self.A02, self.Z20 * T2)
        Ind = K1 + K2i + K3
        
        # Combined output
        H = self.alpha * Dir + (1 - self.alpha) * Ind
        return self.act(H)  # Swish instead of ReLU

# --------------------------------------------------
# SCN-PDE Solver: minimal edits from original MP-PDE
# --------------------------------------------------
class SCNPDEModel(nn.Module):
    def __init__(self, num_nodes, time_steps, feature_dims, hidden_dim):
        super().__init__()
        # Build topological maps
        inc_maps, Z_maps = build_scn_maps(num_nodes, time_steps)
        
        # Encoders with Swish
        self.encoder0 = nn.Sequential(
            nn.Linear(feature_dims['node'], hidden_dim), Swish())
        self.encoder1 = nn.Sequential(
            nn.Linear(feature_dims['edge'], hidden_dim), Swish())
        self.encoder2 = nn.Sequential(
            nn.Linear(feature_dims['triangle'], hidden_dim), Swish())
        self.encoder3 = nn.Sequential(
            nn.Linear(feature_dims['tetra'], hidden_dim), Swish())
        
        # Kernel & decoder
        self.kernel = SCNKernel(hidden_dim, inc_maps, Z_maps)
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, feature_dims['node']), Swish())

    def forward(self, X0, X1, X2, X3):
        X0_enc = self.encoder0(X0)
        X1_enc = self.encoder1(X1)
        X2_enc = self.encoder2(X2)
        X3_enc = self.encoder3(X3)
        H = self.kernel(X0_enc, X1_enc, X2_enc, X3_enc)
        return self.decoder(H)

# --------------------------------------------------
# Utilities: build incidence (Â) and normalization (Z) maps
# --------------------------------------------------
def build_scn_maps(mesh, time_steps):
    # mesh: 2D spatial with .edges (.edge_index), .triangles (.tri_index), .nodes
    # 1) spatial incidences
    A01 = normalize_incidence(mesh.edge_index)
    A02 = normalize_incidence(mesh.tri_to_node_index)
    A12 = normalize_incidence(mesh.tri_to_edge_index)
    # 2) spatio-temporal: extrude triangles->tets over time
    tet_index = extrude_triangles_to_tets(mesh.triangles, time_steps)
    A23 = normalize_incidence(tet_index)
    # compute Z^{-1/2} diagonal matrices for indirect paths
    Z10 = compute_Zinv_sqrt(mesh.edge_index, mesh.num_nodes)
    Z20 = compute_Zinv_sqrt(mesh.tri_to_node_index, mesh.num_nodes)
    Z32 = compute_Zinv_sqrt(tet_index,       mesh.triangles.shape[0]* (time_steps-1))
    return (
        {'0_1': A01, '1_2': A12, '0_2': A02, '2_3': A23},
        {'1_0': Z10, '2_0': Z20, '3_2': Z32}
    )
    

# helper funcs

def normalize_incidence(index):
    # Convert edge/triangle/tet indices to a sparse adjacency matrix A
    # and compute Z^{-1/2} A Z^{-1/2} normalization.
    # assumes index is a [2, num_edges] tensor for adjacency.
    # Returns sparse tensor A_hat.
    from torch_sparse import SparseTensor

    row, col = index
    n = max(row.max(), col.max()) + 1
    adj = SparseTensor(row=row, col=col, sparse_sizes=(n, n))
    deg = adj.sum(dim=1).pow(-0.5)
    deg[deg == float('inf')] = 0
    adj_norm = deg.view(-1, 1) * adj * deg.view(1, -1)
    return adj_norm

def compute_Zinv_sqrt(index, dim):
    # Compute Z^{-1/2} for indirect paths (degree matrix diagonal).
    # `index`: [k, num_simplices] (e.g., edges, triangles)
    # Returns diagonal matrix as a 1D tensor.
    from torch_scatter import scatter_add

    ones = torch.ones(index.size(1), device=index.device)
    deg = scatter_add(ones, index[0], dim_size=dim)  # sum per node
    deg_inv_sqrt = deg.pow(-0.5)
    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
    return deg_inv_sqrt